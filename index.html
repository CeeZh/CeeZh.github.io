<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ce Zhang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Ce Zhang (Âº†ÂÜå)
                  </p>
                  <p>
                    I am a second year Ph.D. student at <a href="https://www.seu.edu.cn/english/">UNC-Chapel Hill</a> working with <a href="https://www.gedasbertasius.com/"> Prof. Gedas Bertasius</a>. Previously, I obtained my Master's degree from <a href="https://www.brown.edu/">Brown Universiy</a> advised by <a href="https://chensun.me/index.html">Prof. Chen Sun</a> in 2023. 
                    Before that, I obtained my Bachelor's degree from <a href="https://www.seu.edu.cn/english/">Southeast University</a> in China in 2020.
                  </p>
                  <p>
                    During my Ph.D, I also interned at Meta AI (Summer 2024, Fall 2024) working with <a href="https://satwikkottur.github.io/">Satwik Kottur</a>.
                  </p>
                  <p>
                    I like music, guitar, basketball and PC games.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:cezhang@cs.unc.edu">Email</a> &nbsp;/&nbsp;
                    <a href="data/CV_Ce_Zhang_UNC_phd.pdf">CV</a> &nbsp;/&nbsp;
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=zGA2ReUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/cezhhh">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://github.com/CeeZh">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/IMG_2231.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/IMG_2231.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I'm broadly interested in Computer Vision, Multimodal learning and Robotics. Currently, I'm mainly working on <b>Video Understanding</b>, with a focus on leveraging <b>foundation models</b> (LLMs, VLMs, etc.) to solve multiple video understanding tasks.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <ul>
                    <li>[02/28/2025] <b>BASKET</b> was accepted to <b>CVPR 2025</b>. Paper, code and data is coming soon. </li>
                    <li>[02/28/2025] Present <a href="https://arxiv.org/pdf/2312.17235.pdf">LLoVi</a> at <a href="https://www.youtube.com/playlist?list=PLvqwYT_ECloZPB2BsBerHXxMpLGr2xuw9">Multimodal Friday</a> hosted by <a href="https://www.twelvelabs.io/">Twelve Labs</a>. </li>
                    <li>[12/05/2024] Present <a href="https://arxiv.org/pdf/2312.17235.pdf">LLoVi</a> at <a href="https://www.youtube.com/watch?v=6jBiKCl5G7o">Ego4D Winter Symposium</a>. </li>
                    <li>[11/22/2024] Received the <span style="color: red;">Outstanding Reviewer Award</span> at <b>ECCV 2024</b>.</li>
                    <li>[09/20/2024] <a href="https://arxiv.org/pdf/2312.17235.pdf">LLoVi</a> was accepted to <b>EMNLP 2024</b>. </li>
                    <li>[05/28/2024] Started internship at Meta FAIR Embodied AI working on visual planning.</li>
                    <li>[08/21/2023] Joined UNC-Chapel Hill as a Ph.D student.</li>
                  </ul>
                </td>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publication</h2>
                </td>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
 
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/videoplan.png" alt="clean-usnob" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ceezh.github.io/">
                    <span class="papertitle">Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction</span>
                  </a>
                  <br>
                  <b>Ce Zhang</b>, Yale Song, Ruta Desai, Michael Louis Iuzzolino, Joseph Tighe, Gedas Bertasius, Satwik Kottur
                  <br>
                  <em>In Submission</em>
                  <p>We introduce VideoPlan, a multimodal large language model optimized for long-horizon visual planning. We introduce Auxiliary Task Augmentation and Multi-token Prediction to enhence the visual planning ability. VideoPlan achieves SOTA performance on COIN and CrossTask for the challenging Visual Planning for Assistance (VPA) task. VideoPlan also achieves competitive performance on the Ego4D Long-term Action Anticipation benchmark.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/basket.png" alt="clean-usnob" width="160" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ceezh.github.io/">
                    <span class="papertitle">BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation</span>
                  </a>
                  <br>
                  Yulu Pan, <b>Ce Zhang</b>, Gedas Bertasius
                  <br>
                  <em>CVPR</em>, 2025
                  <p>We present BASKET, a large-scale basketball video dataset for fine-grained skill estimation. BASKET contains more than 4,400 hours of video capturing 32,232 basketball players from all over the world. We benchmark multiple SOTA video recognition models and reveal that these models struggle to achieve good results on our benchmark.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/llovi.png" alt="clean-usnob" width="160" height="140">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2312.17235.pdf">
                    <span class="papertitle">A Simple LLM Framework for Long-Range Video Question-Answering</span>
                  </a>
                  <br>
                  <b>Ce Zhang</b>*, Taixi Lu*, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, Gedas Bertasius
                  <br>
                  <em>EMNLP</em>, 2024
                  <p>We present LLoVi, a language-based framework for long-range video question-answering (LVQA). LLoVi decomposes LVQA into two stages: (1) visual captioning by a short-term visual captioner, and (2) long-range temporal reasoning by an LLM. We did thorough empirical analysis on our proposed framework. LLoVi achieves state-of-the-art performance on EgoSchema, NExT-QA, IntentQA and NExT-GQA.</p>
                </td>
              </tr>
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/antgpt.png" alt="clean-usnob" width="160" height="95">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2307.16368.pdf">
                    <span class="papertitle">AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?</span>
                  </a>
                  <br>
                  Qi Zhao*, Shijie Wang*, <b>Ce Zhang</b>, Changcheng Fu, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, Chen Sun
                  <br>
                  <em>ICLR</em>, 2024
                  <p>We use discretized action labels to represent videos, then feed the text representations to LLMs for long-term action anticipation. Results on Ego4D, EK-55 and Gaze show that this simple approach is suprisingly effective.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/object_prompt.png" alt="clean-usnob" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2311.00180.pdf">
                    <span class="papertitle">Object-centric Video Representation for Long-term Action Anticipation</span>
                  </a>
                  <br>
                  <b>Ce Zhang</b>*, Changcheng Fu*, Shijie Wang, Nakul Agarwal, Kwonjoon Lee, Chiho Choi, Chen Sun
                  <br>
                  <em>WACV</em>, 2024
                  <p>We proposed ObjectPrompts, an approach
                    to extract task-specific object-centric representations from general-purpose pretrained models without finetuning. We also proposed a Transformer-based architecture to retrieve relevant objects from the past observation for long-term action anticipation.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/gcpc.png" alt="clean-usnob" width="160" height="70">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2307.03406.pdf">
                    <span class="papertitle">Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning</span>
                  </a>
                  <br>
                  Zilai Zeng, <b>Ce Zhang</b>, Shijie Wang, Chen Sun
                  <br>
                  <em>NeurIPS</em>, 2023
                  <p>We investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. GCPC achieves competitive performance on AntMaze, FrankaKitchen and Locomotion.</p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Services</h2>
                  <p>
                    Reviewer: ICCV 2025, CVPR 2025, ECCV 2024 (<span style="color: red;">Outstanding Reviewer Award</span>), ACL Rolling Review (June 2024, Dec. 2024), IEEE TCSVT <br>
                    Organizer: <a href="https://sites.google.com/view/t4v-cvpr24">T4V @ CVPR 2024</a>
                  </p>
                </td>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This webpage is adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's page</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
  </table>
</body>

</html>