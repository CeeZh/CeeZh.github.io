<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ce Zhang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Ce Zhang (Âº†ÂÜå)
                  </p>
                  <p>
                    I am a first year Ph.D. student at <a href="https://www.seu.edu.cn/english/">UNC-Chapel Hill</a>
                    working with <a href="https://www.gedasbertasius.com/"> Prof. Gedas Bertasius</a>. Previously, I
                    obtained my Master's degree from <a href="https://www.brown.edu/">Brown Universiy</a> advised by <a
                      href="https://chensun.me/index.html">Prof. Chen Sun</a> in 2023. During my Master's Program, I was
                    also fortunate to work with <a
                      href="https://scholar.google.com/citations?user=C6Wu8M0AAAAJ&hl=en">Dr. Kwonjoon Lee</a> in a
                    project collaborated with <a href="https://usa.honda-ri.com/">Honda Research Institute
                      USA</a>. Before that, I obtained my Bachelor's degree from <a
                      href="https://www.seu.edu.cn/english/">Southeast University</a> in China in 2020.
                  </p>
                  <p>
                    I like music, sports, PC games, and watching funny videos.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:cezhang@cs.unc.edu">Email</a> &nbsp;/&nbsp;
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp;/&nbsp; -->
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                    <a href="https://github.com/CeeZh">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/IMG_2231.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/IMG_2231.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I'm boradly interested in Computer Vision, Multimodal learning and Robotics. Currently, I'm mainly
                    working on video understanding, with a focus on leveraging foundation models (LLMs, VLMs, etc.) to
                    solve multiple video understanding tasks. I'm also interested in offline decision making, especially
                    learning from videos. I believe the commonsense knowledge encoded in foundation models would help
                    solve robotic tasks faster and more robustly.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/llovi.png" alt="clean-usnob" width="160" height="140">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2312.17235.pdf">
                    <span class="papertitle">A Simple LLM Framework for Long-Range Video Question-Answering</span>
                  </a>
                  <br>
                  Ce Zhang*, Taixi Lu*, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, Gedas Bertasius
                  <br>
                  <em>arXiv</em>, 2023
                  <p>We present LLoVi, a language-based framework for long-range video question-answering (LVQA). LLoVi decomposes LVQA into two stages: (1) visual captioning by a short-term visual captioner, and (2) long-range temporal reasoning by an LLM. We did thorough empirical analysis on our proposed framework. LLoVi achieves state-of-the-art performance on EgoSchema, NExT-QA, IntentQA and NExT-GQA.</p>
                </td>
              </tr>
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/antgpt.png" alt="clean-usnob" width="160" height="95">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2307.16368.pdf">
                    <span class="papertitle">AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?</span>
                  </a>
                  <br>
                  Qi Zhao*, Shijie Wang*, Ce Zhang, Changcheng Fu, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, Chen Sun
                  <br>
                  <em>arXiv</em>, 2023
                  <p>We use discretized action labels to represent videos, then feed the text representations to LLMs for long-term action anticipation. Results on Ego4D, EK-55 and Gaze show that this simple approach is suprisingly effective.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/object_prompt.png" alt="clean-usnob" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2311.00180.pdf">
                    <span class="papertitle">Object-centric Video Representation for Long-term Action Anticipation</span>
                  </a>
                  <br>
                  Ce Zhang*, Changcheng Fu*, Shijie Wang, Nakul Agarwal, Kwonjoon Lee, Chiho Choi, Chen Sun
                  <br>
                  <em>WACV</em>, 2024
                  <p>We proposed ObjectPrompts, an approach
                    to extract task-specific object-centric representations from general-purpose pretrained models without finetuning. We also proposed a Transformer-based architecture to retrieve relevant objects from the past observation for long-term action anticipation.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/gcpc.png" alt="clean-usnob" width="160" height="70">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2307.03406.pdf">
                    <span class="papertitle">Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning</span>
                  </a>
                  <br>
                  Zilai Zeng, Ce Zhang, Shijie Wang, Chen Sun
                  <br>
                  <em>NeurIPS</em>, 2023
                  <p>We investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. GCPC achieves competitive performance on AntMaze, FrankaKitchen and Locomotion.</p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This webpage is adapted from <a href="https://github.com/jonbarron/jonbarron_website"> </a>Jon Barron's page</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>